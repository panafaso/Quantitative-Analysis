{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#  Mount Google Drive to the path so I can have access to google drive in google colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wseH04unD6vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "n = 100 # set the number of words as a limit per file\n",
        "# open the txt file and read it\n",
        "with open('/content/drive/MyDrive/75texts/Οι κόρες της Αφροδίτης - Μιχάλης Πιτένης.pdf.v2.txt', \"r\", encoding='utf8') as file:\n",
        "  single_line = file.read().replace(\"\\n\", \" \") # read the file content and replace line breaks with spaces\n",
        "    # print(single_line)\n",
        "\n",
        "words = single_line.split(\" \") # split the text in single words\n",
        "parts = [words[i:i+n] for i in range(0, len(words), n)]  # divide the word list into parts, each with 'n' words\n",
        "\n",
        "print(\"Initial split:\", parts[0]) # print the first part for checking\n",
        "\n",
        "print(\"All parts:\", len(parts)) # print the total number of parts\n",
        "print(\"Last part:\", len(parts[-1])) # print the number of words in the last part\n",
        "\n",
        "all_files = []  # create an empty list to save the txt file segments\n",
        "# loop all parts except the last one\n",
        "for i in range(len(parts[:-1])):\n",
        "    file_sin_line = \"\"   # make an empty string to build the text of the current part\n",
        "    for j in parts[i]:  # loop each word in the current part\n",
        "      file_sin_line = file_sin_line + \" \" + j  # add each word with a space\n",
        "    all_files.append(file_sin_line.strip())  # add the cleaned string to the list\n"
      ],
      "metadata": {
        "id": "bY8bQygXDgJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"./aphrodite_text_splits/\" # make the path in which the txt files will be saved\n",
        "os.makedirs(path, exist_ok=True) # create the directory"
      ],
      "metadata": {
        "id": "APIWt1cbFz96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./aphrodite_text_splits/\"\n",
        "\n",
        "# loop for each small text part we created previously\n",
        "for i, f in enumerate(all_files):\n",
        "  file_name = path+\"file_\"+ str(i) +\".txt\" # create a file name\n",
        "  file = open(file_name, \"w\") # open the file to write to it\n",
        "  file.write(f) # write the text into the file\n",
        "  print(f\"Saved:{file_name}\")"
      ],
      "metadata": {
        "id": "dvokYp9_DmGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate h-point from a list of word frequencies\n",
        "def h_point(frequencies):\n",
        "\n",
        "  # Sorting frequencies in descending order\n",
        "  frequencies.sort(reverse=True)\n",
        "\n",
        "  # Iterating over the list\n",
        "  for i, f_c in enumerate(frequencies, start=1):\n",
        "\n",
        "      # if frequency is equal to rank\n",
        "      if i == f_c:\n",
        "        hpoint=i # match between rank and frequency\n",
        "        break\n",
        "\n",
        "      # if frequency is less than rank\n",
        "      elif i>f_c:\n",
        "        fr1 = frequencies[i-2] # previous frequency\n",
        "        r1 = i-1   # previous rank\n",
        "        fr2 = f_c   # current frequency\n",
        "        r2 = i     # current rank\n",
        "        hpoint = ((fr1 * r2) - (fr2 * r1)) / ((r2 - r1) + (fr1 - fr2))\n",
        "        break\n",
        "\n",
        "  return hpoint\n",
        "\n",
        "# calculating H-Index\n",
        "def H_index(frequencies):\n",
        "\n",
        "  # sorting in ascending order\n",
        "  frequencies.sort()\n",
        "  print(frequencies)\n",
        "  # iterating over the list\n",
        "  for i, freq in enumerate(frequencies):\n",
        "\n",
        "      # finding current result\n",
        "      result = len(frequencies) - i\n",
        "\n",
        "      # if result is less than or equal to cited then return result\n",
        "      if result <= freq:\n",
        "          return result\n",
        "\n",
        "  return 0\n",
        "\n",
        "# calculate R1\n",
        "def R1(frequencies, N, h_index, h_point):\n",
        "  cumsum=0  # sum of frequencies up to h-index\n",
        "\n",
        "  for i in range(0,h_index):\n",
        "      cumsum+=frequencies[i]\n",
        "\n",
        "# apply the R1 formula using total tokens N and h_point\n",
        "  R1_res = 1 - ((cumsum/N) - (pow(h_point,2) / (2*N)))\n",
        "\n",
        "  return R1_res\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "# calculate entropy of word frequency distribution\n",
        "def Entropy(frequencies, N):\n",
        "  entr_sum = 0  # initialize entropy sum\n",
        "\n",
        "  for i in range(len(frequencies)):\n",
        "      p_i = frequencies[i]/N  # probability of word i\n",
        "      entr_sum += p_i * math.log2(p_i)\n",
        "\n",
        "  ent = - entr_sum   # final entropy - negative sum\n",
        "\n",
        "  return ent\n",
        "\n",
        "# calculate the Lambda - measures variation in word usage frequency\n",
        "def Lambda_func(frequencies, N):\n",
        "  L_sum = 0   # initialize sum\n",
        "\n",
        "  # Loop for frequency pairs\n",
        "  for i in range(len(frequencies)-1):\n",
        "      res = pow((pow(frequencies[i]-frequencies[i+1], 2) + 1),1/2)\n",
        "      L_sum += res\n",
        "\n",
        "  # final Lambda value using log scaling\n",
        "  lam = (L_sum * math.log10(N)) / N\n",
        "  return lam\n",
        "\n",
        "# calculate the average token length weighted by frequency\n",
        "def ATL(dictionary, N):\n",
        "  len_sum = 0\n",
        "\n",
        "  for i, (key, value) in enumerate(dictionary.items()):\n",
        "      len_sum += len(key)*value  # word length * number of times it appears\n",
        "\n",
        "  atl = len_sum / N  # average token length = total length / total tokens\n",
        "\n",
        "  return atl"
      ],
      "metadata": {
        "id": "A95mevi7GQ82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "all_indeces = []  # make a list to store all index values for each text part\n",
        "\n",
        "rr_data = {} # make an empty dictionary to save (V, N) pairs for each file\n",
        "\n",
        "# loop each text segment and its index\n",
        "for no, line in enumerate(all_files):\n",
        "  file_name = f\"file_{no:03d}.txt\"  # create filename with format no:03d\n",
        "\n",
        "  # Create an empty dictionary\n",
        "  d = dict()\n",
        "\n",
        "  # Remove the leading spaces and newline character\n",
        "  line = line.strip()\n",
        "\n",
        "  # Convert the characters in line to lowercase to avoid case mismatch\n",
        "  line = line.lower()\n",
        "\n",
        "  # replace \" ' \" with [space]\n",
        "  line = line.replace(\"\\'\", \" \")\n",
        "\n",
        "  # replace \" - \" with [space]\n",
        "  line = line.replace(\"-\", \" \")\n",
        "\n",
        "  # Split the line into words\n",
        "  words = line.split(\" \")\n",
        "\n",
        "  # Iterate over each word in line\n",
        "  for word in words:\n",
        "    if word == \"\":\n",
        "      continue\n",
        "      # Check for words in () or []\n",
        "    if word.startswith('[') or word.startswith('('):\n",
        "      word = word[1:]\n",
        "    if word.endswith(']') or word.endswith(')'):\n",
        "      word = word[:-1]\n",
        "\n",
        "    # Check for words that end with punctuation\n",
        "    if word.endswith('.') or word.endswith(',') or word.endswith(';') or word.endswith(':'):\n",
        "      word = word[:-1]\n",
        "\n",
        "    # Check if the word is already in dictionary\n",
        "    if word in d:\n",
        "      # Increment count of word by 1\n",
        "      d[word] = d[word] + 1\n",
        "    else:\n",
        "      # Add the word to dictionary with count 1\n",
        "      d[word] = 1\n",
        "\n",
        "  # Sorting dictionary by word frequency in descending order\n",
        "  sortedDict = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  num_tokens = 0  # Total number of word tokens\n",
        "  frequencies = []  # make a list of word frequencies\n",
        "\n",
        "  # collect token and type stats\n",
        "  for i, (key, value) in enumerate(sortedDict.items(), start=1):\n",
        "    num_tokens += value\n",
        "    num_types = i   # i becomes the total number of unique types\n",
        "    frequencies.append(value)\n",
        "\n",
        "  V = num_types  # number of unique words\n",
        "  N = num_tokens  # Total number of words (tokens)\n",
        "  TTR = V/N  # Type-Token Ratio\n",
        "\n",
        "  h_p = h_point(frequencies) # compute h-point\n",
        "\n",
        "  h_i = H_index(frequencies)  # compute h-index\n",
        "\n",
        "  r1 = R1(frequencies, N, h_i, h_p) # compute R1\n",
        "\n",
        "  entr = Entropy(frequencies, N) # compute entropy\n",
        "\n",
        "  lamb = Lambda_func(frequencies, N) # compute lambda\n",
        "\n",
        "  atl = ATL(sortedDict, N) # compute average token length\n",
        "\n",
        "  rr_data[file_name] = (V, N) # save (V, N) for this file to calculate Repeat Rate later in the question 9\n",
        "\n",
        "  # print all the index values for this file\n",
        "  print(f\"{file_name} → TTR: {TTR:}, H_point: {h_p:}, R1: {r1:}, Entropy: {entr:}, Lambda: {lamb:}, ATL: {atl:}\")\n",
        "\n",
        "  # store the results in a list for this file\n",
        "  file_index = [no, TTR, h_p, r1, entr, lamb, atl]\n",
        "  all_indeces.append(file_index) # add this file's results to the main list"
      ],
      "metadata": {
        "id": "vWsdQGMlGWxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# create a zip file named \"aphrodite.zip\" from the \"aphrodite_text_splits\" folder to give it to the professor (requirement)\n",
        "shutil.make_archive(\"aphrodite\", 'zip', \"/content/aphrodite_text_splits\")"
      ],
      "metadata": {
        "id": "cHfUUUqDWGdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# put the column names for the output CSV and dataframe\n",
        "labels = [\"File_name\", \"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "print(labels) # print the column headers\n",
        "print(all_indeces[2]) # print the index values of the 3rd file - preview\n",
        "\n",
        "import csv  # import the CSV module for file writing\n",
        "\n",
        "csv_filename = \"indices_results.csv\"\n",
        "\n",
        "# open the file in write mode\n",
        "with open(csv_filename, 'w', newline=\"\") as file:\n",
        "    csvwriter = csv.writer(file) # create a csvwriter object\n",
        "    csvwriter.writerow(labels) # write the header\n",
        "    csvwriter.writerows(all_indeces) # write the rest of the data\n",
        "\n",
        "# create a pandas DataFrame from the list of results\n",
        "df = pd.DataFrame(all_indeces, columns=labels)  # create the columns based on label names\n",
        "print(df)"
      ],
      "metadata": {
        "id": "gNesgB04IDNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"indices_results.csv\"\n",
        "\n",
        "# read the CSV file into a DataFrame\n",
        "texts_results = pd.read_csv(file_path, sep=',', decimal=\",\") # 'sep' : the column separator  | 'decimal': character used for decimal numbers\n",
        "\n",
        "# print the shape of the Dataframe (rows, columns)\n",
        "print(\"shape:\\t\", texts_results.shape)\n",
        "texts_results.head() # show olny the first 5 rows of the df    # I made this format of Dataframe for clear preview - visibility for me, like your quantitative_indexes_lab.ipynb"
      ],
      "metadata": {
        "id": "Md0Hl5k-XG0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# make a list of text indices I want to analyze\n",
        "indicators = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "\n",
        "# loop for each index to create boxplots and show basic stats\n",
        "for col in indicators:\n",
        "  print(f\"\\n-Δείκτης: {col}\")\n",
        "\n",
        "  # print basic stats: min, max, median\n",
        "  print(\"min\\t\", df[col].min())\n",
        "  print(\"max\\t\", df[col].max())\n",
        "  print(\"median\\t\", df[col].median())\n",
        "\n",
        "  # calculate the 25th and 75th percentiles\n",
        "  q75, q25 = np.percentile(df[col], [75 ,25])\n",
        "  iqr = q75 - q25  # interquartile range\n",
        "\n",
        "  # print quartiles and IQR\n",
        "  print(\"q25\\t\", q25)\n",
        "  print(\"q75\\t\", q75)\n",
        "  print(\"iqr\\t\", iqr)\n",
        "\n",
        "  # print bounds for detecting outliers\n",
        "  print(\"‘minimum’\\t\", q25 - 1.5 * iqr)\n",
        "  print(\"‘maximum’\\t\", q75 + 1.5 * iqr)\n",
        "\n",
        "  # create a vertical boxplot for the current index\n",
        "  df.boxplot(column=[col])\n",
        "  plt.title(f\"Κάθετο Boxplot για τον δείκτη {col}\")\n",
        "  plt.show()\n",
        "\n",
        "  # create a horizontal boxplot for the same index\n",
        "  df.boxplot(column=[col], vert=False)\n",
        "  plt.title(f\"Οριζόντιο Boxplot για τον δείκτη {col}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "cz7DPIIYaAmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the mode of the H_point\n",
        "mode_hpoint = df[\"H_point\"].mode()\n",
        "print(\"Επικρατούσα τιμή (mode) για τον δείκτη h-point:\", mode_hpoint.values)\n",
        "\n",
        "# print how many times each unique H_point value appears\n",
        "print(df[\"H_point\"].value_counts())\n",
        "\n",
        "# create a bar chart to visualize the frequency of each H_point value (optional - for my preview)\n",
        "df[\"H_point\"].value_counts().plot(kind='bar', grid=True)"
      ],
      "metadata": {
        "id": "G33SXRIpbHR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the CSV file which contains all index results\n",
        "df = pd.read_csv(\"indices_results.csv\")\n",
        "\n",
        "# make a list of columns to find and analyze outliers\n",
        "columns = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "\n",
        "# loop each column to check for outliers\n",
        "for col in columns:\n",
        "  print(f\"\\n-Outliers για τον δείκτη: {col}\")\n",
        "\n",
        "  # calculate Q1, Q3, and IQR\n",
        "  q1 = df[col].quantile(0.25)\n",
        "  q3 = df[col].quantile(0.75)\n",
        "  iqr = q3 - q1\n",
        "\n",
        "  # define lower and upper bounds for detecting outliers\n",
        "  minimum = q1 - 1.5 * iqr\n",
        "  maximum = q3 + 1.5 * iqr\n",
        "\n",
        "  print(f\"Q1: {q1:}, Q3: {q3:}, IQR: {iqr:}\")\n",
        "  print(f\"Κατώτερο όριο: {minimum:}\")\n",
        "  print(f\"Ανώτερο όριο: {maximum:}\")\n",
        "\n",
        "  # filter the DataFrame to find outliers (outside the bounds)\n",
        "  outliers = df[(df[col] < minimum) | (df[col] > maximum)]\n",
        "\n",
        "  # results based on detected outliers\n",
        "  if len(outliers)==0:\n",
        "    print(\"Δεν βρέθηκαν outliers για αυτόν τον δείκτη.\")\n",
        "  else:\n",
        "    print(f\"Βρέθηκαν {len(outliers)} outliers:\")\n",
        "    print(outliers[['File_name', col]])"
      ],
      "metadata": {
        "id": "mFJ5LswPb4kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the CSV file with the calculated indices\n",
        "df = pd.read_csv(\"indices_results.csv\")\n",
        "\n",
        "indicators = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "\n",
        "# make a list to save the final statistical results\n",
        "results = []\n",
        "\n",
        "# loop for each indicator\n",
        "for col in indicators:\n",
        "  q1 = df[col].quantile(0.25)\n",
        "  q3 = df[col].quantile(0.75)\n",
        "  iqr = q3 - q1\n",
        "\n",
        "  lower = q1 - 1.5 * iqr\n",
        "  upper = q3 + 1.5 * iqr\n",
        "\n",
        "  # filter the data to exclude outliers\n",
        "  no_outliers = df[(df[col] >= lower) & (df[col] <= upper)]\n",
        "\n",
        "  # calculate statistics on the cleaned data\n",
        "  mean_value = no_outliers[col].mean()\n",
        "  median_value = no_outliers[col].median()\n",
        "  stand_dev_value = no_outliers[col].std()\n",
        "\n",
        "  # save the results in a dictionary\n",
        "  results.append({\n",
        "      \"Δείκτης\": col,\n",
        "      \"Μέση τιμή\": float(mean_value),\n",
        "      \"Διάμεση τιμή\": median_value,\n",
        "      \"Τυπική απόκλιση\": stand_dev_value\n",
        "      })\n",
        "\n",
        "print(results)\n",
        "\n",
        "for result in results:\n",
        "  print(\n",
        "      f\"\\nΔείκτης: {result['Δείκτης']}\\n\"\n",
        "      f\"Μέση τιμή: {result['Μέση τιμή']}\\n\"\n",
        "      f\"Διάμεση τιμή: {result['Διάμεση τιμή']}\\n\"\n",
        "      f\"Τυπική απόκλιση: {result['Τυπική απόκλιση']}\\n\"\n",
        "      )"
      ],
      "metadata": {
        "id": "wmTgDyBHc8HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this method I used sns.histplot with kde=True to plot both the histogram and the KDE together.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "columns = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "\n",
        "# loop for each index\n",
        "for col in columns:\n",
        "  plt.figure(figsize=(10, 5))   # create a new figure with specified size\n",
        "  sns.histplot(df[col], bins=20, kde=True)   # create a histogram with a KDE (Kernel Density Estimation)\n",
        "  plt.title(f\"Ιστόγραμμα και συνάρτηση πυκνότητας πιθανότητας: {col}\")\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel(\"Πυκνότητα\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FnU0gJWrdf1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (another method - optional) In this method I first plot the histogram and then add the KDE line using sns.kdeplot (I used documentation from https://seaborn.pydata.org/generated/seaborn.kdeplot.html)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "columns = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "\n",
        "# Loop for each index\n",
        "for col in columns:\n",
        "    plt.figure(figsize=(10, 5))  # Create new figure\n",
        "\n",
        "    # Histogram that shows density (not count)\n",
        "    sns.histplot(df[col], bins=20, stat=\"density\")\n",
        "\n",
        "    # KDE line in orange\n",
        "    sns.kdeplot(df[col], color=\"orange\")\n",
        "\n",
        "    # Titles and labels\n",
        "    plt.title(f\"Ιστόγραμμα και συνάρτηση πυκνότητας πιθανότητας: {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Πυκνότητα\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d1Sdo0NFs-MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the calculated **skewness**, most indices show values between -0.5 and 0.5, which, according to the rule of thumb, indicate fairly symmetrical distributions. Especially, TTR, H-point, R1, Entropy, Lambda and Average Token Length have skewness values within this range, which mean that their distributions are approximately symmetric, with no significant skew to the right or left. The positive skewness, that is noticed on H-point, R1 and ATL, implies a slight right-skewed distribution, while the negative one for TTR, Entropy and Lambda implies a slight left-skew. Overall, it is evident that the data are fairly symmetrical, without significant asymmetry.\n",
        "\n",
        "In terms of **kurtosis** analysis, most indices have kurtosis close to zero and do not significantly exceed the threshold defined by the rule of thumb (|kurtosis| > 4 x √(6/703) ≈ 0.3695). This shows that the distributions are mainly mesokurtic which means that they have tails similar to a normal distribution. Especially, TTR, R1, Entropy, Lambda and ATL have mesokurtic distribution, meaning that there is a standard concentration of values without extreme outliers. However, H-point has a platykurtic distribution, meaning that its distribution has lighter tails relative to a normal distribution with fewer extreme values. Overall, the kurtosis results confirm that the data don't show heavy-tailed or extremely light-tailed distribution.\n",
        "\n",
        "Regarding the **KDE** plot, for most indices (TTR, Entropy, Lambda and ATL) display a single peak and curve which indicate the **normal distribution.** However, H-point and R1 shows more irregular and multimodal shape which means that there is a deviation from normal distribution. Overall, the indices approximate the normal distrbution with minor deviations for specific cases ( H-point, R1).\n",
        "\n",
        "In conclusion, the majority of the indices approximate normality, with fairly symmetrical shapes, mesokurtic and smooth KDE curves. Minor deviations (H point and R1) were noticed, but overall the data show consistency with normal distribution.\n",
        "\n",
        "**You can see the following and above code results/analysis (question 7 and 8) to validate the above conclusion:**"
      ],
      "metadata": {
        "id": "Fj0zDYFfIvd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kurtosis, skew    # I followed your code (professor's code) in Descriptive_Stats_lab.ipynb\n",
        "import seaborn as sns\n",
        "\n",
        "columns = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"]\n",
        "\n",
        "# Loop for each column - index\n",
        "for col in columns:\n",
        "  print(f\" - {col} \")\n",
        "\n",
        "  # take the data for each column\n",
        "  part1 = df[col]\n",
        "\n",
        "  # do basic statistics\n",
        "  res = part1.describe()\n",
        "  print(res)\n",
        "\n",
        "  part2 = part1 / res.get('count') # normalize data\n",
        "\n",
        "  # compute skewness\n",
        "  skewness = skew(part2)\n",
        "  print(\"\\nskewness:\", skewness)\n",
        "\n",
        "  if skewness < 0:\n",
        "    print(\"****left-skewed distribution (negatively skewed)****\\n\")\n",
        "  if skewness > 0:\n",
        "    print(\"****right-skewed distribution (positively skewed)****\\n\")"
      ],
      "metadata": {
        "id": "_CJoVwzFPdvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kurtosis, skew\n",
        "\n",
        "columns = [\"TTR\", \"H_point\", \"R1\", \"Entropy\", \"Lambda\", \"ATL\"] # define the index names\n",
        "\n",
        "# Loop for finding kurtosis for each column\n",
        "for col in columns:\n",
        "  print(f\"- {col} \")\n",
        "\n",
        "  part1 = df[col] # take the data for each column\n",
        "  res = part1.describe() # compute basic descriptive statistics\n",
        "  print(res)\n",
        "  part2 = part1/res.get('count') # normalize the data\n",
        "\n",
        "  kurtosis_value = kurtosis(part1) # compute the kurtosis\n",
        "  print(\"\\nkurtosis:\\t\", kurtosis_value)\n",
        "\n",
        "  thr = 4 * (6 / res.get('count'))**0.5 # calculate the rule of thumb threshold\n",
        "  print(\"rule of thumb threshold:\\t\", thr)\n",
        "\n",
        "  if abs(kurtosis_value) > thr:  # if the absolute kurtosis exceeds the threshold\n",
        "    if kurtosis_value > 0:\n",
        "      print(\"****leptokurtic(heavy tails)****\\n\")\n",
        "    else:\n",
        "      print(\"****platykurtic(light tails)****\\n\")\n",
        "  else:\n",
        "    print(\"****mesokurtic (normal tails)****\\n\")"
      ],
      "metadata": {
        "id": "qJTarKLskX1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary to save the final Repeat Rate for each file\n",
        "rr_results = {}\n",
        "\n",
        "# loop for each file and its (V, N) values\n",
        "for file_name, (V, N) in rr_data.items():\n",
        "    RR = (N - V) / V   # Repeat Rate: measures the level of repetition that exists in the text\n",
        "    rr_results[file_name] = RR  # put the RR in the correct dictionary\n",
        "    print(f\"{file_name} - Repeat Rate (RR): {RR:}\")"
      ],
      "metadata": {
        "id": "hSAFYi2hjuh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}